---
title: "uncertaintyR: Quantifying Predictive Uncertainty"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{uncertaintyR: Quantifying Predictive Uncertainty}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Motivation

Understanding the range of plausible outcomes surrounding a point prediction is critical for
decision making. `uncertaintyR` offers a unified interface to compute predictive intervals via
bootstrap, jackknife, Bayesian, and conformal methods, making it easier to compare and visualise
uncertainty estimates across modelling approaches.

## Getting Started

```{r setup}
library(uncertaintyR)
library(ggplot2)
```

We begin with a linear regression model fitted to the `mtcars` dataset.

```{r fit-model}
mod <- lm(mpg ~ wt + hp, data = mtcars)
u_fit <- fit_uncertainty(
  mod,
  method = c("bootstrap", "jackknife", "conformal"),
  B = 200,
  seed = 2025
)
u_fit
```

## Prediction Intervals

```{r predict-interval}
intervals <- predict_interval(
  u_fit,
  newdata = mtcars,
  level = 0.95,
  seed = 123
)
head(intervals)
```

```{r plot-interval}
plot_uncertainty(intervals, actual = mtcars$mpg, title = "Uncertainty bands for mpg")
```

## Correctness Check

The point predictions returned by `predict_interval()` match the base predictions from `predict.lm()`
(up to numerical tolerance).

```{r correctness}
base_preds <- predict(mod, newdata = mtcars)
all.equal(
  base_preds,
  dplyr::filter(intervals, method == "bootstrap")$estimate,
  tolerance = 1e-8
)
```

## Efficiency Benchmark

We benchmark the runtime cost of computing intervals versus base predictions. The bootstrap method
incurs extra overhead due to resampling, while conformal and jackknife provide alternatives with
different trade-offs.

```{r benchmark, warning = FALSE}
bench::mark(
  base = predict(mod, newdata = mtcars),
  bootstrap = predict_interval(u_fit, mtcars[1:10, ], method = "bootstrap", B = 50),
  conformal = predict_interval(u_fit, mtcars[1:10, ], method = "conformal"),
  iterations = 5,
  check = FALSE
)
```

## Extending to Other Models

Any model that implements `predict()` and `update()` can be wrapped. For example, gradient boosted
trees trained via `xgboost` or models produced by `caret::train()` work out of the box provided the
training data can be recovered via `model.frame()`.

## Simulated Nonlinear Example

```{r simulated}
set.seed(101)
sim_data <- tibble::tibble(
  x1 = runif(200, -2, 2),
  x2 = runif(200, -2, 2),
  noise = rnorm(200, sd = 0.5),
  y = 3 + 2 * sin(x1) - 1.5 * x2^2 + noise
)

sim_mod <- lm(y ~ poly(x1, 3) + poly(x2, 2), data = sim_data)
sim_fit <- fit_uncertainty(
  sim_mod,
  X = sim_data[, c("x1", "x2")],
  y = sim_data$y,
  method = c("bayes", "conformal"),
  B = 150
)

grid <- tidyr::expand_grid(
  x1 = seq(-2, 2, length.out = 20),
  x2 = seq(-2, 2, length.out = 20)
)

sim_intervals <- predict_interval(sim_fit, as.data.frame(grid))
head(sim_intervals)
```

```{r simulated-plot}
ggplot2::ggplot(
  dplyr::filter(sim_intervals, method == "bayes"),
  ggplot2::aes(x = estimate)
) +
  ggplot2::geom_histogram(fill = "#377EB8", colour = "white") +
  ggplot2::labs(
    title = "Distribution of Bayesian predictions on grid",
    x = "Prediction",
    y = "Count"
  ) +
  ggplot2::theme_minimal()
```

## Session Info

```{r session-info}
sessionInfo()
```

